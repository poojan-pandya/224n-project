{"cells":[{"cell_type":"markdown","source":["Install necessary packages and set up wandb"],"metadata":{"id":"FOd2kLwiynlC"}},{"cell_type":"code","source":["!pip install pytorch_lightning"],"metadata":{"id":"GQFgz4qRvv3T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install nlp"],"metadata":{"id":"ShKljacpv6x8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"pIjkHrapwIH2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NKzmNQtSiO0s","executionInfo":{"status":"ok","timestamp":1677838457673,"user_tz":480,"elapsed":23039,"user":{"displayName":"Priya Khandelwal","userId":"03711845755022262656"}},"outputId":"ec773fc5-bbc1-44dd-9da1-55e954a04142"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import argparse\n","import glob\n","import os\n","import json\n","import time\n","import logging\n","import random\n","import re\n","from itertools import chain\n","from string import punctuation\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import pytorch_lightning as pl\n","from torch.utils.data import Dataset, DataLoader\n","from pytorch_lightning.loggers import WandbLogger\n","from nlp import load_metric\n","\n","from transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup\n",")"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/CS\\ 224N\\ Project\n","! ls # verify that you are in the right directory"],"metadata":{"id":"pnOgCOo6zdyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = pd.read_csv('aita_train_set.csv')\n","valid_data = pd.read_csv('aita_valid_set.csv')\n","test_data = pd.read_csv('aita_test_set.csv')"],"metadata":{"id":"mvmT8CI3zSiK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(valid_data['comments'].values.tolist())"],"metadata":{"id":"OHcT0FJ_8HM0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valid_data.iloc[4]"],"metadata":{"id":"MiEs5uIy4jgk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class aita(Dataset):\n","    def __init__(self, tokenizer, data_df, num_samples, input_length, output_length, print_text=False):         \n","        self.dataset = data_df\n","        if num_samples:\n","            self.dataset = self.dataset.select(list(range(0, num_samples)))\n","        self.input_length = input_length\n","        self.tokenizer = tokenizer\n","        self.output_length = output_length\n","        self.print_text = print_text\n","  \n","    def __len__(self):\n","        return len(self.dataset)\n","    \n","    def convert_to_features(self, example_batch):\n","        # Tokenize contexts and questions (as pairs of inputs)\n","        \n","        input_ = example_batch['text'].values.tolist()\n","        target_ = example_batch['comments'].values.tolist()\n","        \n","        source = self.tokenizer.batch_encode_plus([input_], max_length=self.input_length, \n","                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n","        \n","        targets = self.tokenizer.batch_encode_plus([target_], max_length=self.output_length, \n","                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n","    \n","        return source, targets\n","  \n","    def __getitem__(self, index):\n","        source, targets = self.convert_to_features(self.dataset.iloc[index])\n","        \n","        source_ids = source[\"input_ids\"].squeeze()\n","        target_ids = targets[\"input_ids\"].squeeze()\n","\n","        src_mask    = source[\"attention_mask\"].squeeze()\n","        target_mask = targets[\"attention_mask\"].squeeze()\n","\n","        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}"],"metadata":{"id":"8npTa1da0K4x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"id":"kknaUztV9Sk7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test that the dataset class is okay"],"metadata":{"id":"EjlbgWlw9Bu0"}},{"cell_type":"code","source":["tokenizer = T5Tokenizer.from_pretrained('t5-small')\n","dataset = aita(tokenizer, valid_data, None, 512, 50, True)\n","len(dataset)"],"metadata":{"id":"Nr0t_MCm9Ddd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define model"],"metadata":{"id":"lQTH8-EkDAn8"}},{"cell_type":"code","source":["def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","set_seed(42)"],"metadata":{"id":"En4bouRvDABP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class T5FineTuner(pl.LightningModule):\n","    def __init__(self, hparams):\n","        print(hparams)\n","        super(T5FineTuner, self).__init__()\n","        self.hparams = hparams       \n","        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n","        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n","        self.rouge_metric = load_metric('rouge') \n","        \n","        if self.hparams.freeze_embeds:\n","            self.freeze_embeds()\n","        if self.hparams.freeze_encoder:\n","            self.freeze_params(self.model.get_encoder())\n","            #assert_all_frozen(self.model.get_encoder())\n","            \n","            \n","        n_observations_per_split = {\n","            \"train\": self.hparams.n_train,\n","            \"validation\": self.hparams.n_val,\n","            \"test\": self.hparams.n_test,\n","        }\n","        self.n_obs = {k: v if v >= 0 else None for k, v in n_observations_per_split.items()}\n","        \n","    \n","    def freeze_params(self, model):\n","        for par in model.parameters():\n","            par.requires_grad = False\n","            \n","            \n","    def freeze_embeds(self):\n","        \"\"\"Freeze token embeddings and positional embeddings for bart, just token embeddings for t5.\"\"\"\n","        try:\n","            self.freeze_params(self.model.model.shared)\n","            for d in [self.model.model.encoder, self.model.model.decoder]:\n","                freeze_params(d.embed_positions)\n","                freeze_params(d.embed_tokens)\n","        except AttributeError:\n","            self.freeze_params(self.model.shared)\n","            for d in [self.model.encoder, self.model.decoder]:\n","                self.freeze_params(d.embed_tokens)\n","    \n","    def lmap(self, f, x):\n","        \"\"\"list(map(f, x))\"\"\"\n","        return list(map(f, x))\n","    \n","\n","    def is_logger(self):\n","        return self.trainer.proc_rank <= 0\n","    \n","    def parse_score(self, result):\n","        return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}\n","        \n","    def forward(\n","      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n","  ):\n","        return self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            lm_labels=lm_labels,\n","    )\n","\n","    def _step(self, batch):\n","        lm_labels = batch[\"target_ids\"]\n","        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","        outputs = self(\n","            input_ids=batch[\"source_ids\"],\n","            attention_mask=batch[\"source_mask\"],\n","            lm_labels=lm_labels,\n","            decoder_attention_mask=batch['target_mask']\n","        )\n","\n","        loss = outputs[0]\n","\n","        return loss\n","    \n","    \n","    def ids_to_clean_text(self, generated_ids):\n","        gen_text = self.tokenizer.batch_decode(\n","            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","        )\n","        return self.lmap(str.strip, gen_text)\n","    \n","    \n","    def _generative_step(self, batch) :\n","        \n","        t0 = time.time()\n","        \n","        generated_ids = self.model.generate(\n","            batch[\"source_ids\"],\n","            attention_mask=batch[\"source_mask\"],\n","            use_cache=True,\n","            decoder_attention_mask=batch['target_mask'],\n","            max_length=150, \n","            num_beams=2,\n","            repetition_penalty=2.5, \n","            length_penalty=1.0, \n","            early_stopping=True\n","        )\n","        preds = self.ids_to_clean_text(generated_ids)\n","        target = self.ids_to_clean_text(batch[\"target_ids\"])\n","            \n","        gen_time = (time.time() - t0) / batch[\"source_ids\"].shape[0]  \n","    \n","        loss = self._step(batch)\n","        base_metrics = {'val_loss': loss}\n","#         rouge: Dict = self.calc_generative_metrics(preds, target)\n","        summ_len = np.mean(self.lmap(len, generated_ids))\n","        base_metrics.update(gen_time=gen_time, gen_len=summ_len, preds=preds, target=target)\n","        self.rouge_metric.add_batch(preds, target)\n","        \n","#         rouge_results = self.rouge_metric.compute() \n","#         rouge_dict = self.parse_score(rouge_results)\n","#         base_metrics.update(rouge1=rouge_dict['rouge1'], rougeL=rouge_dict['rougeL'])\n","        \n","        return base_metrics\n","    \n","\n","    def training_step(self, batch, batch_idx):\n","        loss = self._step(batch)\n","\n","        tensorboard_logs = {\"train_loss\": loss}\n","        return {\"loss\": loss, \"log\": tensorboard_logs}\n","  \n","    def training_epoch_end(self, outputs):\n","        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n","        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n","        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","    def validation_step(self, batch, batch_idx):\n","        return self._generative_step(batch)\n","    \n","  \n","    def validation_epoch_end(self, outputs):\n","        \n","        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n","        tensorboard_logs = {\"val_loss\": avg_loss}\n","        \n","        rouge_results = self.rouge_metric.compute() \n","        rouge_dict = self.parse_score(rouge_results)\n","    \n","        tensorboard_logs.update(rouge1=rouge_dict['rouge1'], rougeL=rouge_dict['rougeL'])\n","        \n","        ## Clear out the lists for next epoch\n","        self.target_gen= []\n","        self.prediction_gen=[]\n","        return {\"avg_val_loss\": avg_loss, \n","                \"rouge1\" : rouge_results['rouge1'],\n","                \"rougeL\" : rouge_results['rougeL'],\n","                \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","    def configure_optimizers(self):\n","        \"Prepare optimizer and schedule (linear warmup and decay)\"\n","\n","        model = self.model\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.hparams.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n","        self.opt = optimizer\n","        return [optimizer]\n","  \n","    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None, using_native_amp=False):\n","        if self.trainer.use_tpu:\n","            xm.optimizer_step(optimizer)\n","        else:\n","            optimizer.step()\n","        optimizer.zero_grad()\n","        self.lr_scheduler.step()\n","  \n","    def get_tqdm_dict(self):\n","        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n","\n","        return tqdm_dict\n","    \n","    def train_dataloader(self):   \n","        n_samples = self.n_obs['train']\n","        train_dataset = get_dataset(tokenizer=self.tokenizer, data_df=train_data, num_samples=n_samples, args=self.hparams)\n","        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n","        t_total = (\n","            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n","            // self.hparams.gradient_accumulation_steps\n","            * float(self.hparams.num_train_epochs)\n","        )\n","        scheduler = get_linear_schedule_with_warmup(\n","            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n","        )\n","        self.lr_scheduler = scheduler\n","        return dataloader\n","\n","    def val_dataloader(self):\n","        n_samples = self.n_obs['validation']\n","        validation_dataset = get_dataset(tokenizer=self.tokenizer, data_df=valid_data, num_samples=n_samples, args=self.hparams)\n","        \n","        return DataLoader(validation_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n","    \n","    \n","    def test_dataloader(self):\n","        n_samples = self.n_obs['test']\n","        test_dataset = get_dataset(tokenizer=self.tokenizer, data_df=test_data, num_samples=n_samples, args=self.hparams)\n","        \n","        return DataLoader(test_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"],"metadata":{"id":"7Yf7zcZXDPfr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logger = logging.getLogger(__name__)\n","\n","class LoggingCallback(pl.Callback):\n","    def on_validation_end(self, trainer, pl_module):\n","        logger.info(\"***** Validation results *****\")\n","        if pl_module.is_logger():\n","            metrics = trainer.callback_metrics\n","            # Log results\n","            for key in sorted(metrics):\n","                if key not in [\"log\", \"progress_bar\"]:\n","                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","\n","    def on_test_end(self, trainer, pl_module):\n","        logger.info(\"***** Test results *****\")\n","\n","        if pl_module.is_logger():\n","            metrics = trainer.callback_metrics\n","\n","            # Log and save results to file\n","            output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n","            with open(output_test_results_file, \"w\") as writer:\n","                for key in sorted(metrics):\n","                    if key not in [\"log\", \"progress_bar\"]:\n","                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","                        writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"],"metadata":{"id":"AM0VrfCbFesc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Args for model + dataset"],"metadata":{"id":"RsZxlDIu-iT4"}},{"cell_type":"code","source":["args_dict = dict(\n","    output_dir=\"t5_checkpoints\", # path to save the checkpoints\n","    model_name_or_path='t5-small',\n","    tokenizer_name_or_path='t5-small',\n","    max_input_length=512,\n","    max_output_length=50,\n","    freeze_encoder=False,\n","    freeze_embeds=False,\n","    learning_rate=3e-4,\n","    weight_decay=0.0,\n","    adam_epsilon=1e-8,\n","    warmup_steps=0,\n","    train_batch_size=4,\n","    eval_batch_size=4,\n","    num_train_epochs=2,\n","    gradient_accumulation_steps=8,\n","    n_gpu=1,\n","    resume_from_checkpoint=None, \n","    val_check_interval = 0.05, \n","    n_val=1000,\n","    n_train=-1,\n","    n_test=-1,\n","    early_stop_callback=False,\n","    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n","    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n","    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n","    seed=42,\n",")\n","args = argparse.Namespace(**args_dict)"],"metadata":{"id":"nTv76Qkt-k-5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_uTZ-qwDHQCe","executionInfo":{"status":"ok","timestamp":1677841731622,"user_tz":480,"elapsed":3,"user":{"displayName":"Priya Khandelwal","userId":"03711845755022262656"}},"outputId":"a846dfde-c145-4da7-d648-de31e0514b1a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Namespace(adam_epsilon=1e-08, early_stop_callback=False, eval_batch_size=4, fp_16=False, freeze_embeds=False, freeze_encoder=False, gradient_accumulation_steps=8, learning_rate=0.0003, max_grad_norm=1.0, max_input_length=512, max_output_length=50, model_name_or_path='t5-small', n_gpu=1, n_test=-1, n_train=-1, n_val=1000, num_train_epochs=2, opt_level='O1', output_dir='t5_checkpoints', resume_from_checkpoint=None, seed=42, tokenizer_name_or_path='t5-small', train_batch_size=4, val_check_interval=0.05, warmup_steps=0, weight_decay=0.0)"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["## Define Checkpoint function\n","checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","    dirpath=args.output_dir, monitor=\"val_loss\", mode=\"min\", save_top_k=3)\n","\n","## If resuming from checkpoint, add an arg resume_from_checkpoint\n","train_params = dict(\n","    accumulate_grad_batches=args.gradient_accumulation_steps,\n","    gpus=args.n_gpu,\n","    max_epochs=args.num_train_epochs,\n","    early_stop_callback=False,\n","    precision= 16 if args.fp_16 else 32,\n","    amp_level=args.opt_level,\n","    resume_from_checkpoint=args.resume_from_checkpoint,\n","    gradient_clip_val=args.max_grad_norm,\n","    checkpoint_callback=checkpoint_callback,\n","    val_check_interval=args.val_check_interval,\n","    logger=wandb_logger,\n","    #callbacks=[LoggingCallback()],\n",")"],"metadata":{"id":"0TNm3gAxBn_C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = T5FineTuner(args)"],"metadata":{"id":"DRixAP9uGSu4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bkvd7HB3hgFL"},"source":["Get average length of comment so we can guide our model on how long we want it's answer to be"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2893,"status":"ok","timestamp":1677747992870,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"},"user_tz":480},"id":"uxx8GIzrhkRZ","outputId":"c696cc00-42be-4147-94f7-e84f153e77a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["49.232252930626984\n","71.40746107343492\n"]}],"source":["avg_words = df['comments'].str.split().str.len().mean()\n","avg_words_text = df['text'].str.split().str.len().mean()\n","print(avg_words)\n","print(avg_words_text)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
