{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2vionRRzF_O"
      },
      "source": [
        "### GAN Attempt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JADRaL5b276C"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESRs9D8S2gd3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Setup\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txGRbtkd4IWq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "%cd drive/MyDrive/CS\\ 224N/CS\\ 224N\\ Project\n",
        "%ls # verify that you are in the right directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RgiUTBezH2V"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Define the generator (use the pre-trained BART implementation)\n",
        "\"\"\"\n",
        "\n",
        "# bart-base checkpoint pre-trained on our dataset\n",
        "# (can also try generically pre-trained bart base)\n",
        "model_dir = 'bart-base-checkpoint-204000'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "netG = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
        "print(netG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBPd1v0Q34Jn"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Define transformer discriminator\n",
        "\"\"\"\n",
        "\n",
        "nc = 1\n",
        "ndf = 64\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        \n",
        "        # Transformer Encoder\n",
        "        self.upsample = nn.Upsample(size=(64))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=8)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        #input shape: (batch_size, seq_len, d_model)\n",
        "        upsampled_input = self.upsample(input)\n",
        "        transformer_output = self.transformer_encoder(upsampled_input) \n",
        "        discriminator_output = self.classifier(transformer_output.mean(dim=1)) #(batch_size, 1)\n",
        "        \n",
        "        return discriminator_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4fElnOc_aAR"
      },
      "outputs": [],
      "source": [
        "ngpu = 1\n",
        "netD = Discriminator(ngpu).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "sentiment = pipeline(model='finiteautomata/bertweet-base-sentiment-analysis')\n",
        "#sentiment = AutoModel.from_pretrained('finiteautomata/bertweet-base-sentiment-analysis')"
      ],
      "metadata": {
        "id": "dtD26URRQSlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment_tokens(comment):\n",
        "  max_input_length = 128\n",
        "  if len(comment) > 128:\n",
        "    comment = comment[:128]\n",
        "  comment_sentiment = sentiment([comment])[0]\n",
        "  comment_sentiment = comment_sentiment['label'] + ': ' + str(comment_sentiment['score'])\n",
        "  comment_sentiment_tokens = tokenizer(comment_sentiment, max_length=128, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "  return comment_sentiment_tokens['input_ids'].tolist()"
      ],
      "metadata": {
        "id": "P5GaX9dBQYRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLLWvDY-7M96"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Loss functions and optimizers\n",
        "\"\"\"\n",
        "# Size of generator input\n",
        "nz = 512\n",
        "# Optim params\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA0gynyNom7B"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('aita_train_set.csv')[['text', 'comments']]\n",
        "valid_df = pd.read_csv('aita_valid_set.csv')[['text', 'comments']]\n",
        "test_df = pd.read_csv('aita_test_set.csv')[['text', 'comments']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf4cgIQL9uCA"
      },
      "outputs": [],
      "source": [
        "train_data_txt = Dataset.from_pandas(train_df)\n",
        "validation_data_txt = Dataset.from_pandas(valid_df)\n",
        "test_data_txt = Dataset.from_pandas(test_df)\n",
        "print(train_data_txt)\n",
        "print(validation_data_txt)\n",
        "print(test_data_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvBpN-KNAJ4x"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Preprocess\n",
        "\"\"\"\n",
        "\n",
        "encoder_max_length = 256  # changed from 256\n",
        "decoder_max_length = 64  # changed from 64\n",
        "\n",
        "def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n",
        "    source, target = batch[\"text\"], batch[\"comments\"]\n",
        "    source_tokenized = tokenizer(\n",
        "        source, padding=\"max_length\", truncation=True, max_length=max_source_length, return_tensors=\"pt\"\n",
        "    )\n",
        "    target_tokenized = tokenizer(\n",
        "        target, padding=\"max_length\", truncation=True, max_length=max_target_length, return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    batch = {k: v for k, v in source_tokenized.items()}\n",
        "    # Ignore padding in the loss\n",
        "    batch[\"labels\"] = [\n",
        "        [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
        "        for l in target_tokenized[\"input_ids\"]\n",
        "    ]\n",
        "    return batch\n",
        "\n",
        "\n",
        "train_data = train_data_txt.map(\n",
        "    lambda batch: batch_tokenize_preprocess(\n",
        "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
        "    ),\n",
        "    batched=True,\n",
        "    remove_columns=train_data_txt.column_names,\n",
        ")\n",
        "\n",
        "validation_data = validation_data_txt.map(\n",
        "    lambda batch: batch_tokenize_preprocess(\n",
        "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
        "    ),\n",
        "    batched=True,\n",
        "    remove_columns=validation_data_txt.column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRueMmaYFamD"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVc3xgHXFdk4"
      },
      "outputs": [],
      "source": [
        "print(len(train_data[0]['input_ids']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYSn2PZcA4Pa"
      },
      "outputs": [],
      "source": [
        "fixed_validation_index = 17\n",
        "fixed_validation_inputs = valid_df.iloc[fixed_validation_index]['text']\n",
        "fixed_validation_data = tokenizer(fixed_validation_inputs, max_length=512, padding='max_length', truncation=True, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTM_7TRDEuwr"
      },
      "outputs": [],
      "source": [
        "batch_size=4\n",
        "dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                         shuffle=True)\n",
        "for i, data in enumerate(dataloader, 0):\n",
        "  print(torch.stack(data['attention_mask']).shape)\n",
        "  break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7CIqDPACJV9"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "num_epochs = 1\n",
        "max_input_length = 512\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # todo: batch this/use a dataloader\n",
        "    for i in range(len(validation_data)):\n",
        "        data = validation_data[i]\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        #print(len(data['labels']))\n",
        "        #real_cpu = torch.tensor(data['labels'], dtype=torch.float32)\n",
        "        #real_cpu = data['labels']\n",
        "        #print(real_cpu.shape)\n",
        "        # real_cpu = real_cpu.unsqueeze(0)\n",
        "        # real_cpu = real_cpu.unsqueeze(0)\n",
        "        # real_cpu = real_cpu.unsqueeze(0)\n",
        "        #print(real_cpu.shape)\n",
        "        sentiment_tokens = get_sentiment_tokens(str(valid_df.iloc[i]['comments']))\n",
        "        real_cpu = torch.tensor(data['input_ids'] + data['labels'] + sentiment_tokens[0], dtype=torch.float32)\n",
        "        real_cpu = real_cpu.view(1, 1, 64 + 256 + 128)\n",
        "        #real_cpu = real_cpu.view(1, 1, 64) #these are the comment tokens\n",
        "        #print(real_cpu.shape)\n",
        "        #print(real_cpu.shape)\n",
        "        real_cpu = real_cpu.to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "\n",
        "        #discriminator will train off of true comments in the real batch pass\n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1) \n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        # noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        # print(inputs['input_ids'].shape)\n",
        "        # output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=64)\n",
        "        # Generate fake image batch with G\n",
        "\n",
        "        inputs = valid_df.iloc[i]['text']\n",
        "        data = tokenizer(inputs, max_length=max_input_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "        fake = netG.generate(**data, num_beams=8, do_sample=True, min_length=10, max_length=64) #generate a fake comment\n",
        "\n",
        "        decoded_fake_comment = tokenizer.batch_decode(fake, skip_special_tokens=True)\n",
        "        sentiment_tokens = get_sentiment_tokens(decoded_fake_comment[0])\n",
        "        inp_tensor_1 = torch.tensor(sentiment_tokens[0], dtype=torch.long)\n",
        "        fake = torch.cat((fake, inp_tensor_1), dim=1)\n",
        "        inp_tensor = torch.tensor(data['input_ids'], dtype=torch.long)\n",
        "        fake = torch.cat((inp_tensor, fake), dim=1)\n",
        "        label.fill_(fake_label)\n",
        "        # Classify all fake batch with D\n",
        "        #print(fake.shape)\n",
        "        fake = fake.type(torch.float32)\n",
        "        fake = fake.view(1, 1, -1)\n",
        "        #print(fake.shape)\n",
        "        #fake = correct_to_64(fake)\n",
        "        #print(fake.shape)\n",
        "        fake = fake.detach().to(device)\n",
        "        output = netD(fake).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Compute error of D as sum over the fake and the real batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        output = netD(fake).view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 5 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(validation_data),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "        if iters == 5: netG.save_pretrained('blueberry/blueberry-initial-save')\n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters != 0 and iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_data)-1)):\n",
        "            netG.save_pretrained('blueberry/blueberry-halfway')\n",
        "            with torch.no_grad():\n",
        "                fake = netG.generate(**fixed_validation_data, num_beams=8, do_sample=True, min_length=10, max_length=64).detach()\n",
        "            # img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "        iters += 1\n",
        "\n",
        "netG.save_pretrained('blueberry/blueberry-full')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}