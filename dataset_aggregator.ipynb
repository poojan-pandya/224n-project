{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMkcZVk49ZLVnl1k/3y9e/1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":76,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1yYuBKo-ruvY","executionInfo":{"status":"ok","timestamp":1677916767413,"user_tz":480,"elapsed":1619,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"outputId":"2fcec116-7d92-4c21-fa78-72be5d6b110b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[Errno 2] No such file or directory: 'drive/MyDrive/CS 224N/CS 224N Project'\n","/content/drive/MyDrive/CS 224N/CS 224N Project\n","'224N Project Brainstorm.gdoc'\t        drive\n","'224N Project Helpful Tutorials.gdoc'  'first proposal OLD'\n","'224N Project Milestone Notes.gdoc'     gpt2-baseline.ipynb\n"," aita_clean.csv\t\t\t        logs\n"," aita_comments.csv\t\t       'Reddit Scraper.ipynb'\n"," aita_test_set.csv\t\t        results\n"," aita_train_set.csv\t\t        T5_Attempt_2.ipynb\n"," aita_valid_set.csv\t\t        T5-baseline.ipynb\n"," bart-baseline-attempt2.ipynb\t        t5_checkpoints\n"," bert-baseline.ipynb\t\t        train.pt\n"," csvs\t\t\t\t        wandb\n"," dataset_agg.ipynb\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/CS\\ 224N/CS\\ 224N\\ Project\n","! ls # verify that you are in the right directory"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","path = os.listdir('csvs')\n","print(path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U68RG61TtCRV","executionInfo":{"status":"ok","timestamp":1677916769312,"user_tz":480,"elapsed":241,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"outputId":"66c1d4ea-6330-4922-b56d-ce97452f442d"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["['comments_batch_8273.csv', 'comments_batch_11473.csv', 'comments_batch_4067.csv', 'comments_batch_15737.csv', 'comments_batch_19402.csv', 'comments_batch_21869.csv', 'comments_batch_60700.csv', 'comments_batch_62058.csv', 'comments_batch_24666.csv', 'comments_batch_62270.csv', 'comments_batch_63000.csv', 'comments_batch_25587.csv', 'comments_batch_63037.csv', 'comments_batch_64000.csv', 'comments_batch_65000.csv', 'comments_batch_26957.csv', 'comments_batch_33231.csv', 'comments_batch_65660.csv', 'comments_batch_66000.csv', 'comments_batch_27485.csv', 'comments_batch_66593.csv', 'comments_batch_33688.csv', 'comments_batch_67000.csv', 'comments_batch_68000.csv', 'comments_batch_69000.csv', 'comments_batch_69361.csv', 'comments_batch_70000.csv', 'comments_batch_37064.csv', 'comments_batch_71000.csv', 'comments_batch_72000.csv', 'comments_batch_73000.csv', 'comments_batch_74000.csv', 'comments_batch_75000.csv', 'comments_batch_76000.csv', 'comments_batch_77000.csv', 'comments_batch_78000.csv', 'comments_batch_79000.csv', 'comments_batch_32000.csv', 'comments_batch_42558.csv', 'comments_batch_47966.csv', 'comments_batch_80000.csv', 'comments_batch_81000.csv', 'comments_batch_82000.csv', 'comments_batch_83000.csv', 'comments_batch_84000.csv', 'comments_batch_60000.csv', 'comments_batch_85000.csv', 'comments_batch_86000.csv', 'comments_batch_87000.csv', 'comments_batch_88000.csv', 'comments_batch_89000.csv', 'comments_batch_90000.csv', 'comments_batch_91000.csv', 'comments_batch_92000.csv', 'comments_batch_93000.csv', 'comments_batch_94000.csv', 'comments_batch_95000.csv', 'comments_batch_96000.csv', 'comments_batch_97000.csv']\n"]}]},{"cell_type":"code","source":["df = pd.DataFrame()\n","# %cd csvs\n","# ! ls\n","for i, f in enumerate(path):\n","  df_new = pd.read_csv(f)\n","  # df_new = pd.read_csv(os.path.join(os.getcwd(), f))\n","\n","  df = pd.concat([df, df_new])\n","\n","# remove duplicate ids\n","df = df.drop_duplicates(subset=['id'])\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5d0skmGmsCeR","executionInfo":{"status":"ok","timestamp":1677749359449,"user_tz":480,"elapsed":17811,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"outputId":"3bba8159-e00a-4e77-a8a2-9d8b5ef973ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CS 224N/CS 224N Project/csvs\n","comments_batch_11473.csv  comments_batch_63037.csv  comments_batch_80000.csv\n","comments_batch_15737.csv  comments_batch_64000.csv  comments_batch_81000.csv\n","comments_batch_19402.csv  comments_batch_65000.csv  comments_batch_82000.csv\n","comments_batch_21869.csv  comments_batch_65660.csv  comments_batch_8273.csv\n","comments_batch_24666.csv  comments_batch_66000.csv  comments_batch_83000.csv\n","comments_batch_25587.csv  comments_batch_66593.csv  comments_batch_84000.csv\n","comments_batch_26957.csv  comments_batch_67000.csv  comments_batch_85000.csv\n","comments_batch_27485.csv  comments_batch_68000.csv  comments_batch_86000.csv\n","comments_batch_32000.csv  comments_batch_69000.csv  comments_batch_87000.csv\n","comments_batch_33231.csv  comments_batch_69361.csv  comments_batch_88000.csv\n","comments_batch_33688.csv  comments_batch_70000.csv  comments_batch_89000.csv\n","comments_batch_37064.csv  comments_batch_71000.csv  comments_batch_90000.csv\n","comments_batch_4067.csv   comments_batch_72000.csv  comments_batch_91000.csv\n","comments_batch_42558.csv  comments_batch_73000.csv  comments_batch_92000.csv\n","comments_batch_47966.csv  comments_batch_74000.csv  comments_batch_93000.csv\n","comments_batch_60000.csv  comments_batch_75000.csv  comments_batch_94000.csv\n","comments_batch_60700.csv  comments_batch_76000.csv  comments_batch_95000.csv\n","comments_batch_62058.csv  comments_batch_77000.csv  comments_batch_96000.csv\n","comments_batch_62270.csv  comments_batch_78000.csv  comments_batch_97000.csv\n","comments_batch_63000.csv  comments_batch_79000.csv\n","       id                                           comments\n","0  9m1gfn  NTA. That being said, I think that there are w...\n","1  9m1ihv  NTA while your motives are far from pure ultim...\n","2  9m1w76  You were the asshole here, but I don't think y...\n","3  9m2m9k  One person in the street isn't a protest, in m...\n","4  9m31uc  I'd say it's borderline asshole behaviour. You...\n"]}]},{"cell_type":"code","source":["print(len(df))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fd6kkjv1xcW0","executionInfo":{"status":"ok","timestamp":1677749359450,"user_tz":480,"elapsed":4,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"outputId":"b602ae7d-023f-43bc-b4c5-3e9e47126468"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["87215\n"]}]},{"cell_type":"code","source":["# Remove any elements that are [deleted] from the dataframe\n","# also remove [removed] posts --> these are posts the mods deleted\n","df = df[~df['comments'].str.contains('\\[deleted\\]')]\n","print(len(df))\n","df = df[~df['comments'].str.contains('\\[removed\\]')]\n","print(len(df))\n","\n","df = df[~df['comments'].str.contains('\\[contact the moderator')]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjgLJLGGyieB","executionInfo":{"status":"ok","timestamp":1677916743262,"user_tz":480,"elapsed":170,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"outputId":"c0dc404f-be94-4aaa-e933-a5c8fbbf57a9"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["83929\n","83929\n","83610\n"]}]},{"cell_type":"code","source":["# %cd ..\n","df_clean = pd.read_csv(\"aita_clean.csv\")\n","print(len(df_clean))\n","df_combined = pd.merge(df, df_clean, on='id', how='inner')\n","print(len(df_combined))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gsIIAJo5xdH6","executionInfo":{"status":"ok","timestamp":1677749377558,"user_tz":480,"elapsed":6484,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"outputId":"7c362686-c4e2-40bc-8eb6-acb1a68fbfee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CS 224N/CS 224N Project\n","97628\n","83929\n"]}]},{"cell_type":"code","source":["# save the new data frame in a file called \"aita_comments.csv\"\n","df_combined.to_csv(\"/content/drive/MyDrive/CS 224N/CS 224N Project/aita_comments.csv\", index=False) "],"metadata":{"id":"0Ib0itN6zQeL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conduct final preprocessing on our dataset to structure it appropriately for our generative model"],"metadata":{"id":"Rz1je-sWKFEM"}},{"cell_type":"code","source":["import os\n","import pandas as pd"],"metadata":{"id":"B2zZZy_xKEwj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"aita_comments.csv\")\n","df = df[~df['comments'].str.contains('\\[deleted\\]')]\n","print(len(df))\n","df = df[~df['comments'].str.contains('\\[removed\\]')]\n","print(len(df))\n","\n","df = df[~df['comments'].str.contains('\\[contact the moderator')]"],"metadata":{"id":"mvwA8h7xKQL3","executionInfo":{"status":"ok","timestamp":1677916896944,"user_tz":480,"elapsed":3746,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb86e06b-1a81-407a-a33f-4a5dd25f49f9"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["83929\n","83929\n"]}]},{"cell_type":"code","source":["print(len(df))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cvpjfGtUWNfq","executionInfo":{"status":"ok","timestamp":1677916897271,"user_tz":480,"elapsed":3,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"outputId":"a64c62f9-47bd-4605-8217-6384de039dd5"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["83610\n"]}]},{"cell_type":"code","source":["df[\"text\"] = df[\"title\"].astype(str) +\". \"+ df[\"body\"].astype(str)\n","\n","# create a one hot vector for the verdicts\n","# [\"asshole\", \"everyone sucks\", \"no assholes here\", \"not the asshole\"]\n","# this gets changed to [\"v_yta\", \"v_esh\", \"v_nah\", \"v_nta\"] --> v stands for verdict and the verdict after is abbreviated\n","one_hot = pd.get_dummies(df['verdict'])\n","one_hot = one_hot.rename(columns = {'asshole': 'v_yta', 'everyone sucks': 'v_esh', 'no assholes here': 'v_nah', 'not the asshole': 'v_nta'})\n","\n","df = pd.concat([df, one_hot], axis=1)\n","\n","df = df[[\"text\", \"comments\", \"is_asshole\", \"v_yta\", \"v_esh\", \"v_nah\", \"v_nta\"]] # drop unused columns \n"],"metadata":{"id":"zRgk13KHKVy2","executionInfo":{"status":"ok","timestamp":1677916898554,"user_tz":480,"elapsed":547,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["# normalize text length based on average and mean length of posts + title combo\n","avg_words = df['comments'].str.split().str.len().mean()\n","avg_words_text = df['text'].str.split().str.len().mean()\n","\n","print(f'Average comment length is: ${avg_words}')\n","print(f'Average text length is: ${avg_words_text}')\n","\n","median_words = df['comments'].str.split().str.len().median()\n","median_words_text = df['text'].str.split().str.len().median()\n","print(f'Median comment length is: ${median_words}')\n","print(f'Median text length is: ${median_words_text}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pL3apiESMRwG","executionInfo":{"status":"ok","timestamp":1677916820452,"user_tz":480,"elapsed":13847,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"outputId":"7eb3db44-0ea7-49cc-e09e-caa36c69f44d"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["Average comment length is: $49.08097117569669\n","Average text length is: $348.58258581509386\n","Median comment length is: $37.0\n","Median text length is: $320.0\n"]}]},{"cell_type":"code","source":["cutoff = 350\n","num_long_comments = (df['text'].apply(lambda x: len(x.split(' '))) > cutoff).sum()\n","print(f'The number of comments that are truncated with cutoff: {cutoff} and its percentage of the total dataset')\n","print(num_long_comments)\n","print(num_long_comments / len(df))\n","# if we were to set our cap at 400, these are the number of comments that'd be \n","# affected. We also display the percent of the total that will get truncated\n","cutoff = 400\n","num_long_comments = (df['text'].apply(lambda x: len(x.split())) > cutoff).sum()\n","print(f'The number of comments that are truncated with cutoff: {cutoff} and its percentage of the total dataset')\n","print(num_long_comments)\n","print(num_long_comments / len(df))\n","\n","cutoff = 450\n","num_long_comments = (df['text'].apply(lambda x: len(x.split())) > cutoff).sum()\n","print(f'The number of comments that are truncated with cutoff: {cutoff} and its percentage of the total dataset')\n","print(num_long_comments)\n","print(num_long_comments / len(df))\n","\n","cutoff = 500\n","num_long_comments = (df['text'].apply(lambda x: len(x.split())) > cutoff).sum()\n","print(f'The number of comments that are truncated with cutoff: {cutoff} and its percentage of the total dataset')\n","print(num_long_comments)\n","print(num_long_comments / len(df))\n","\n","cutoff = 550\n","num_long_comments = (df['text'].apply(lambda x: len(x.split())) > cutoff).sum()\n","print(f'The number of comments that are truncated with cutoff: {cutoff} and its percentage of the total dataset')\n","print(num_long_comments)\n","print(num_long_comments / len(df))\n","\n","cutoff = 600\n","num_long_comments = (df['text'].apply(lambda x: len(x.split())) > cutoff).sum()\n","print(f'The number of comments that are truncated with cutoff: {cutoff} and its percentage of the total dataset')\n","print(num_long_comments)\n","print(num_long_comments / len(df))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zhu5Sy50OtaT","executionInfo":{"status":"ok","timestamp":1677916840332,"user_tz":480,"elapsed":19883,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"outputId":"94756209-83a5-4882-867e-647971c3ec99"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of comments that are truncated with cutoff: 350 and its percentage of the total dataset\n","36195\n","0.43290276282741297\n","The number of comments that are truncated with cutoff: 400 and its percentage of the total dataset\n","28934\n","0.34605908384164574\n","The number of comments that are truncated with cutoff: 450 and its percentage of the total dataset\n","22404\n","0.2679583781844277\n","The number of comments that are truncated with cutoff: 500 and its percentage of the total dataset\n","16956\n","0.20279870828848223\n","The number of comments that are truncated with cutoff: 550 and its percentage of the total dataset\n","11788\n","0.14098792010525057\n","The number of comments that are truncated with cutoff: 600 and its percentage of the total dataset\n","4949\n","0.059191484272216245\n"]}]},{"cell_type":"markdown","source":["We chose 508 as the cutoff because the T5 baseline can support at most 512 words. We wanted to include the last 4 words as the prompt \"Am I the Asshole\" because our research found that including a prompt at the end of the text sample greatly helped generative models generate text. \n","\n","508 is also a healthy cutoff because less than 1/5 of our training data will be truncated, it's 19.335% to be exact. \n"],"metadata":{"id":"zA6cHQ5JSAAz"}},{"cell_type":"code","source":["print(len(df))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eagFZDBsWTYr","executionInfo":{"status":"ok","timestamp":1677916913293,"user_tz":480,"elapsed":139,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"outputId":"929ce86f-cfba-4c97-fb56-34ad95d3edf2"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["83610\n"]}]},{"cell_type":"code","source":["cutoff = 508\n","num_long_comments = (df['text'].apply(lambda x: len(x.split())) > cutoff).sum()\n","print(f'The number of comments that are truncated with cutoff: {cutoff} and its percentage of the total dataset')\n","print(num_long_comments)\n","print(num_long_comments / len(df))\n","\n","# Truncate all the strings in the dataset longer than cutoff \n","df['text'] = df['text'].apply(lambda x: ' '.join(x.split(maxsplit=cutoff)[:cutoff]))\n","\n","\n","\n","# add a prompt to help generative model with prompt engineering \n","question = \". Am I the asshole?\"\n","df[\"text\"] = df[\"text\"].astype(str) + question\n","\n","avg_words_text_final = df['text'].str.split().str.len().mean()\n","print(f'Average text length is: ${avg_words_text_final}')\n","\n","median_words_text_final = df['text'].str.split().str.len().median()\n","print(f'Median text length is: ${median_words_text_final}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFO65emjPFj5","executionInfo":{"status":"ok","timestamp":1677916856133,"user_tz":480,"elapsed":15816,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"outputId":"2a2d83dd-f0fc-4bec-9420-60f2dacc6637"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["The number of comments that are truncated with cutoff: 508 and its percentage of the total dataset\n","16152\n","0.1931826336562612\n","Average text length is: $330.57955986126063\n","Median text length is: $324.0\n"]}]},{"cell_type":"code","source":["print(len(df))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pfsBYeqvWVp5","executionInfo":{"status":"ok","timestamp":1677916856134,"user_tz":480,"elapsed":16,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"outputId":"a8eb7d03-cd91-453a-fd8e-93d24e008a7a"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["83610\n"]}]},{"cell_type":"code","source":["# Print counts of classes\n","# print(f\"Count of the YTA label: {}\")\n","size = len(df)\n","ratios = {}\n","ratios['v_yta'] = df['v_yta'].value_counts()[1] / len(df)\n","ratios['v_esh'] = df['v_esh'].value_counts()[1] / len(df)\n","ratios['v_nah'] = df['v_nah'].value_counts()[1] / len(df)\n","ratios['v_nta'] = df['v_nta'].value_counts()[1] / len(df)\n","\n","ratios"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RiFB_M0xgYYW","executionInfo":{"status":"ok","timestamp":1677916917740,"user_tz":480,"elapsed":148,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"outputId":"be87c016-f46f-4e95-8588-f993ddd23a0a"},"execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'v_yta': 0.2129051548857792,\n"," 'v_esh': 0.05647649802655185,\n"," 'v_nah': 0.12212653988757326,\n"," 'v_nta': 0.6084918072000957}"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["# Sample the test and valid dataset to fit the above distribution\n","test_valid_size = 1000\n","yta_df = df.loc[df['v_yta'] == 1] \n","esh_df = df.loc[df['v_esh'] == 1] \n","nah_df = df.loc[df['v_nah'] == 1] \n","nta_df = df.loc[df['v_nta'] == 1] \n","\n","valid_df_yta = yta_df.sample(n = int(ratios[\"v_yta\"] * 1000))\n","valid_df_esh = esh_df.sample(n = int(ratios[\"v_esh\"] * 1000))\n","valid_df_nah = nah_df.sample(n = int(ratios[\"v_nah\"] * 1000))\n","valid_df_nta = nta_df.sample(n = int(ratios[\"v_nta\"] * 1000))\n","valid_df = pd.concat([valid_df_yta, valid_df_esh, valid_df_nah, valid_df_nta])\n","df = df.loc[~df.index.isin(valid_df.index)]\n","\n","valid_set = valid_df.reset_index(drop=True)\n","df = df.reset_index(drop=True)\n","print(len(df))\n","\n","print(len(valid_set))\n","\n","yta_df = df.loc[df['v_yta'] == 1] \n","esh_df = df.loc[df['v_esh'] == 1] \n","nah_df = df.loc[df['v_nah'] == 1] \n","nta_df = df.loc[df['v_nta'] == 1] \n","\n","test_df_yta = yta_df.sample(n = int(ratios[\"v_yta\"] * 1000))\n","test_df_esh = esh_df.sample(n = int(ratios[\"v_esh\"] * 1000))\n","test_df_nah = nah_df.sample(n = int(ratios[\"v_nah\"] * 1000))\n","test_df_nta = nta_df.sample(n = int(ratios[\"v_nta\"] * 1000))\n","test_df = pd.concat([test_df_yta, test_df_esh, test_df_nah, test_df_nta])\n","df = df.loc[~df.index.isin(test_df.index)]\n","\n","test_set = test_df.reset_index(drop=True)\n","df = df.reset_index(drop=True)\n","print(len(df))\n","print(len(test_set))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8X-9K-_li085","executionInfo":{"status":"ok","timestamp":1677916946571,"user_tz":480,"elapsed":390,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}},"outputId":"9a0a680b-781d-4511-da43-b0ccddb84495"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["82612\n","998\n","81614\n","998\n"]}]},{"cell_type":"code","source":["# #Split validation and test set\n","# valid_set = df.sample(n = 1000)\n","# df = df.loc[~df.index.isin(valid_set.index)]\n","\n","# #Reset the indexes\n","# valid_set = valid_set.reset_index()\n","# df = df.reset_index()\n","\n","# test_set = df.sample(n = 1000)\n","# df = df.loc[~df.index.isin(test_set.index)]\n","\n","# #Reset the indexes\n","# test_set = test_set.reset_index()\n","# df = df.reset_index()"],"metadata":{"id":"dFRMSeolS-Xl","executionInfo":{"status":"ok","timestamp":1677914619925,"user_tz":480,"elapsed":158,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# save the new data frames \"\n","df.to_csv(\"/content/drive/MyDrive/CS 224N/CS 224N Project/aita_train_set.csv\", index=False) \n","valid_set.to_csv(\"/content/drive/MyDrive/CS 224N/CS 224N Project/aita_valid_set.csv\", index=False)\n","test_set.to_csv(\"/content/drive/MyDrive/CS 224N/CS 224N Project/aita_test_set.csv\", index=False)"],"metadata":{"id":"CL87_zDyTKz0","executionInfo":{"status":"ok","timestamp":1677916978764,"user_tz":480,"elapsed":8736,"user":{"displayName":"Kavin Anand","userId":"15878202627142921242"}}},"execution_count":98,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_uxruodugUZM"},"execution_count":null,"outputs":[]}]}