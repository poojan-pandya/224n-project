{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oI_51Rm1Tl59"},"outputs":[],"source":["%pip install datasets transformers rouge-score nltk torch pandas numpy"]},{"cell_type":"code","source":["import transformers\n","from datasets import load_dataset, load_metric"],"metadata":{"id":"tTIxrT_7Tndq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import argparse\n","import glob\n","import os\n","import json\n","import time\n","import logging\n","import random\n","import re\n","from itertools import chain\n","from string import punctuation\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup\n",")"],"metadata":{"id":"jEd1p1Y3TsQM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# %cd drive/MyDrive/CS\\ 224N\\ Project\n","%ls # verify that you are in the right directory"],"metadata":{"id":"WSrYwbfCTuko"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = pd.read_csv('aita_train_set.csv')\n","valid_data = pd.read_csv('aita_valid_set.csv')\n","test_data = pd.read_csv('aita_test_set.csv')"],"metadata":{"id":"moSNv9_kTwMi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valid_data = valid_data[['text', 'comments']]\n","train_data = train_data[['text', 'comments']]\n","test_data = test_data[['text', 'comments']]"],"metadata":{"id":"vquJFGhuTw4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#import pandas as pd\n","#import datasets\n","from datasets import Dataset, DatasetDict\n","\n","tds = Dataset.from_pandas(train_data)\n","vds = Dataset.from_pandas(valid_data)\n","tstds = Dataset.from_pandas(test_data)\n","\n","aita_datasets = DatasetDict()\n","\n","aita_datasets['train'] = tds\n","aita_datasets['validation'] = vds\n","aita_datasets['test'] = tstds\n","\n","print(aita_datasets)"],"metadata":{"id":"hZ6YuoUVT0sn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","import string\n","from transformers import AutoTokenizer"],"metadata":{"id":"NqC-ILLFT3CG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_checkpoint = \"t5-small\"\n","custom_eos = '[EOS]'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","tokenizer.add_special_tokens({'eos_token':custom_eos})"],"metadata":{"id":"cErVOl_fT5WS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prefix = \"summarize: \"\n","\n","max_input_length = 512\n","max_target_length = 50\n","\n","def clean_text(text):\n","  sentences = nltk.sent_tokenize(text.strip())\n","  sentences_cleaned = [s for sent in sentences for s in sent.split(\"\\n\")]\n","  sentences_cleaned_no_titles = [sent for sent in sentences_cleaned\n","                                 if len(sent) > 0 and\n","                                 sent[-1] in string.punctuation]\n","  text_cleaned = \"\\n\".join(sentences_cleaned_no_titles)\n","  return text_cleaned\n","\n","def preprocess_data(examples):\n","  texts_cleaned = [clean_text(text) for text in examples[\"text\"]]\n","  inputs = [prefix + text for text in texts_cleaned]\n","  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","\n","  # Setup the tokenizer for targets\n","  with tokenizer.as_target_tokenizer():\n","    no_eos_labels = examples[\"comments\"]\n","    outputs = [label + custom_eos for label in no_eos_labels]\n","    labels = tokenizer(outputs, max_length=max_target_length, truncation=True)\n","\n","  model_inputs[\"labels\"] = labels[\"input_ids\"]\n","  return model_inputs"],"metadata":{"id":"Njdwb0bST75U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#medium_datasets_cleaned = medium_datasets.filter(lambda example: (len(example['text']) >= 500) and (len(example['title']) >= 20))\n","tokenized_datasets = aita_datasets.map(preprocess_data, batched=True)\n","tokenized_datasets"],"metadata":{"id":"KMI26haCT-Nu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"],"metadata":{"id":"pPV2QG54UAOr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 8\n","model_name = \"t5-small-rationale-generation\"\n","model_dir = f\"{model_name}\"\n","args = Seq2SeqTrainingArguments(\n","    model_dir,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=100,\n","    logging_strategy=\"steps\",\n","    logging_steps=100,\n","    save_strategy=\"steps\",\n","    save_steps=200,\n","    learning_rate=3e-4, #made this higher, originally 4e^-5\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=1,\n","    predict_with_generate=True,\n","    fp16=False, #we don't need this to be true bc we care more about accuracy than fast training\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"rouge1\",\n","    report_to=\"tensorboard\"\n",")"],"metadata":{"id":"mDjp00fmUCDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_collator = DataCollatorForSeq2Seq(tokenizer)"],"metadata":{"id":"0mETQLF5UD9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","metric = load_metric(\"rouge\")\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","    \n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    \n","    # Rouge expects a newline after each sentence\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip()))\n","                      for pred in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) \n","                      for label in decoded_labels]\n","    \n","    # Compute ROUGE scores\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels,\n","                            use_stemmer=True)\n","\n","    # Extract ROUGE f1 scores\n","    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","    \n","    # Add mean generated length to metrics\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)\n","                      for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    \n","    return {k: round(v, 4) for k, v in result.items()}"],"metadata":{"id":"Q4ROV9L0UEh9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%pip install pytorch"],"metadata":{"id":"tJFbU-bfUHGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%pip install tensorboardX"],"metadata":{"id":"vfSLCVLSUK9M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%pip install tensorflow\n","%pip install datetime"],"metadata":{"id":"3tIpAsssUM1v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function that returns an untrained model to be trained\n","def model_init():\n","    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n","    model.generation_config.repetition_penalty = 0.4\n","    model.generation_config.no_repeat_ngram_size=4\n","    print(model.generation_config)\n","    model.resize_token_embeddings(len(tokenizer))\n","    return model\n","\n","trainer = Seq2SeqTrainer(\n","    model_init=model_init,\n","    args=args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"pHxIOzvdUPGi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import datetime, os\n","# Start TensorBoard before training to monitor it in progress\n","%load_ext tensorboard\n","#%reload_ext tensorboard\n","%tensorboard --logdir '{model_dir}'/runs"],"metadata":{"id":"1dvCywlOUPyn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"Q1aoTxNSUSfx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model()"],"metadata":{"id":"eiuqqAOrUVl0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%ls"],"metadata":{"id":"IMgQbawCUZR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"t5-small-rationale-generation/checkpoint-8200\"\n","model_dir = f\"{model_name}\"\n","\n","# tokenizer = AutoTokenizer.from_pretrained('t5-small')\n","# model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n","\n","max_input_length = 512"],"metadata":{"id":"L2H42vyfUrWx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testing = test_data.iloc[400]['comments']\n","testing"],"metadata":{"id":"NVhJ4-vpUtDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = testing\n","\n","inputs = [\"summarize: \" + text]\n","\n","inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n","output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=64)\n","decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)\n","print(decoded_output)\n","#predicted_comment = nltk.sent_tokenize(decoded_output.strip())\n","#decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n","#predicted_comment = nltk.sent_tokenize(decoded_output.strip())\n","\n","#print(predicted_comment)\n","# Session State and Callbacks in Streamlit"],"metadata":{"id":"GGVy4-3cVEzf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WN8J9twFVHIe"},"execution_count":null,"outputs":[]}]}